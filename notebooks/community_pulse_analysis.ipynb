{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommunityPulse AI Agent — End-to-End Walkthrough\n",
    "\n",
    "This notebook demonstrates the complete multi-modal pipeline described in the blog post:\n",
    "\n",
    "1. **Load three data modalities** — Reddit text, Census ACS tabular data, and news headlines\n",
    "2. **BERT sentence embeddings** (Week 1 model)\n",
    "3. **BERTopic / LDA topic modeling** (Week 2 model)\n",
    "4. **LangChain LLM agent synthesis** (Week 3 model)\n",
    "5. **Qualitative validation** — case studies and validation table\n",
    "\n",
    "All cells run in **offline mode** — no API keys required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure the repo root is on the path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 — Load All Three Data Modalities\n",
    "\n",
    "The `CommunityPulseDataLoader` orchestrates three specialised loaders:\n",
    "- `RedditLoader` — text posts from city subreddits\n",
    "- `CensusLoader` — ACS demographic/economic indicators\n",
    "- `NewsLoader` — headlines with pre-computed sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import CommunityPulseDataLoader\n",
    "\n",
    "loader = CommunityPulseDataLoader(\n",
    "    communities=['seattle', 'portland', 'denver'],\n",
    "    offline=True,  # Uses bundled sample CSVs — no API keys needed\n",
    ")\n",
    "data = loader.load_all()\n",
    "\n",
    "print(f\"Reddit posts loaded:    {len(data['reddit'])}\")\n",
    "print(f\"Census rows loaded:     {len(data['census'])}\")\n",
    "print(f\"News headlines loaded:  {len(data['news'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Reddit data\n",
    "data['reddit'][['community', 'title', 'score']].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Census data\n",
    "data['census'][[\n",
    "    'community', 'median_household_income', 'median_rent',\n",
    "    'poverty_rate', 'unemployment_rate', 'bachelor_degree_pct'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview News data\n",
    "data['news'][['community', 'headline', 'sentiment_score']].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure A — Demographic Comparison (Census Data)\n",
    "\n",
    "A grouped bar chart comparing key Census indicators across the three cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = data['census']\n",
    "metrics = ['poverty_rate', 'unemployment_rate', 'bachelor_degree_pct']\n",
    "labels  = ['Poverty Rate (%)', 'Unemployment (%)', \"Bachelor's Degree+ (%)\"]\n",
    "communities = census['community'].tolist()\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "colors = ['#4C72B0', '#DD8452', '#55A868']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for i, (community, color) in enumerate(zip(communities, colors)):\n",
    "    row = census[census['community'] == community].iloc[0]\n",
    "    values = [float(row[m]) for m in metrics]\n",
    "    bars = ax.bar(x + i * width, values, width, label=community, color=color, alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_title('Demographic Comparison Across Communities (Census ACS)', fontsize=13)\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/sample/fig_demographics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure B — News Sentiment Distribution\n",
    "\n",
    "Box plot of raw sentiment scores per community + stacked bar of positive/neutral/negative counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = data['news'].copy()\n",
    "\n",
    "def categorise(s):\n",
    "    if s > 0.2: return 'positive'\n",
    "    if s < -0.2: return 'negative'\n",
    "    return 'neutral'\n",
    "\n",
    "news['category'] = news['sentiment_score'].apply(categorise)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=news, x='community', y='sentiment_score', palette='coolwarm', ax=axes[0])\n",
    "axes[0].axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "axes[0].set_title('News Sentiment Score by Community', fontsize=12)\n",
    "axes[0].set_xlabel('Community')\n",
    "axes[0].set_ylabel('Sentiment Score')\n",
    "\n",
    "# Stacked bar\n",
    "counts = news.groupby(['community', 'category']).size().unstack(fill_value=0)\n",
    "counts = counts.reindex(columns=['positive', 'neutral', 'negative'], fill_value=0)\n",
    "counts.plot(kind='bar', stacked=True, color=['#55A868', '#C0C0C0', '#C44E52'],\n",
    "            ax=axes[1], edgecolor='white')\n",
    "axes[1].set_title('Headlines by Sentiment Category', fontsize=12)\n",
    "axes[1].set_xlabel('Community')\n",
    "axes[1].set_ylabel('Number of Headlines')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.suptitle('Figure B — News Sentiment Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/sample/fig_sentiment.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 — Week 1 Model: BERT Sentence Embeddings\n",
    "\n",
    "We use `all-MiniLM-L6-v2` to encode each Reddit post into a 384-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import BERTEmbedder\n",
    "\n",
    "embedder = BERTEmbedder()  # Loads all-MiniLM-L6-v2 (or uses offline fallback)\n",
    "\n",
    "# Embed all Reddit posts\n",
    "data['reddit'] = embedder.embed_dataframe(data['reddit'], text_col='title')\n",
    "\n",
    "print(f\"Embedding shape (first post): {data['reddit']['embedding'].iloc[0].shape}\")\n",
    "print(f\"Sample embedding (first 5 dims): {data['reddit']['embedding'].iloc[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute community centroid cosine similarities\n",
    "communities = data['reddit']['community'].unique()\n",
    "centroids = {\n",
    "    c: np.mean(np.vstack(data['reddit'][data['reddit']['community'] == c]['embedding']), axis=0)\n",
    "    for c in communities\n",
    "}\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "print(\"Discourse similarity (cosine) between community centroids:\")\n",
    "for i, c1 in enumerate(communities):\n",
    "    for c2 in list(communities)[i+1:]:\n",
    "        sim = cosine_sim(centroids[c1], centroids[c2])\n",
    "        print(f\"  {c1} ↔ {c2}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure C — BERT Embedding Clusters (UMAP 2-D Projection)\n",
    "\n",
    "We reduce 384-d embeddings to 2-D using UMAP and colour-code by community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import umap\n",
    "    embeddings = np.vstack(data['reddit']['embedding'].tolist())\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=5, min_dist=0.3)\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 7))\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    for i, community in enumerate(data['reddit']['community'].unique()):\n",
    "        mask = data['reddit']['community'] == community\n",
    "        ax.scatter(reduced[mask, 0], reduced[mask, 1], label=community,\n",
    "                   s=100, alpha=0.85, color=cmap(i), edgecolors='white', linewidths=0.5)\n",
    "    ax.set_title('Figure C — BERT Embedding Clusters (UMAP 2-D)', fontsize=13)\n",
    "    ax.set_xlabel('UMAP Dim 1')\n",
    "    ax.set_ylabel('UMAP Dim 2')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/sample/fig_umap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\nexcept ImportError:\n",
    "    print('umap-learn not installed — skipping UMAP plot. Install with: pip install umap-learn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 — Week 2 Model: Topic Modeling (BERTopic / LDA)\n",
    "\n",
    "We fit a topic model on the combined corpus. BERTopic is tried first; LDA is the fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import TopicModeler\n",
    "\n",
    "topic_modeler = TopicModeler(n_topics=8, min_topic_size=2)\n",
    "\n",
    "docs = (\n",
    "    data['reddit']['title'].fillna('') + ' ' +\n",
    "    data['reddit']['selftext'].fillna('')\n",
    ").tolist()\n",
    "\n",
    "topic_modeler.fit(docs)\n",
    "\n",
    "# Assign topics to each post\n",
    "data['reddit']['topic'] = topic_modeler.assign_topics(docs)\n",
    "\n",
    "print('Topic model backend:', topic_modeler._backend)\n",
    "print()\n",
    "print(topic_modeler.topic_summary_table().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure D — Topic Distribution per Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import plot_topic_distribution\n",
    "plot_topic_distribution(data['reddit'], topic_modeler, save_path='../data/sample/fig_topics.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 — Week 3 Model: LangChain Agent Synthesis\n",
    "\n",
    "The LangChain agent wraps the upstream models as tools and answers natural-language queries.\n",
    "Without an `OPENAI_API_KEY`, it uses a deterministic offline mode that assembles answers from tool outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import CommunityPulseAgent\n",
    "import os\n",
    "\n",
    "agent = CommunityPulseAgent(\n",
    "    embedder=embedder,\n",
    "    topic_modeler=topic_modeler,\n",
    "    data=data,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY', ''),  # empty = offline mode\n",
    ")\n",
    "\n",
    "result = agent.run(\n",
    "    \"Compare Seattle, Portland, and Denver: which city faces the most acute social challenges \"\n",
    "    \"based on news sentiment, Reddit discussion topics, and demographic data?\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual tool calls — inspect the raw evidence\n",
    "for city in ['Seattle', 'Portland', 'Denver']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {city}\")\n",
    "    print('='*50)\n",
    "    print('Sentiment:', agent.get_community_sentiment(city))\n",
    "    print('Topics:   ', agent.get_top_topics(city))\n",
    "    print('Demographics:', agent.get_demographic_profile(city)['profile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 — Qualitative Validation\n",
    "\n",
    "`CommunityPulseAnalyser` joins all three modalities into a scorecard and generates narrative case studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import CommunityPulseAnalyser\n",
    "\n",
    "analyser = CommunityPulseAnalyser(data=data, topic_modeler=topic_modeler)\n",
    "\n",
    "# Numeric scorecard\n",
    "scorecard = analyser.community_summary_table()\n",
    "display(scorecard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative case studies\n",
    "for city in ['Seattle', 'Portland', 'Denver']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  CASE STUDY: {city}\")\n",
    "    print('='*60)\n",
    "    print(analyser.generate_case_study(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Figure E — Multi-Modal Scorecard Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise key metrics for side-by-side heatmap\n",
    "scorecard_plot = scorecard.set_index('community')[[\n",
    "    'mean_news_sentiment', 'poverty_rate_pct', 'unemployment_rate_pct',\n",
    "    'bachelor_degree_pct', 'avg_post_score'\n",
    "]].copy()\n",
    "\n",
    "# Normalise each column to [0, 1]\n",
    "scorecard_norm = (scorecard_plot - scorecard_plot.min()) / (scorecard_plot.max() - scorecard_plot.min() + 1e-9)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "sns.heatmap(\n",
    "    scorecard_norm.T,\n",
    "    annot=scorecard_plot.T.round(2),\n",
    "    fmt='g',\n",
    "    cmap='RdYlGn',\n",
    "    linewidths=0.5,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Normalised score (0=worst, 1=best)'}\n",
    ")\n",
    "ax.set_title('Figure E — Multi-Modal Community Scorecard Heatmap', fontsize=13)\n",
    "ax.set_xlabel('Community')\n",
    "ax.set_yticklabels([\n",
    "    'News Sentiment', 'Poverty Rate', 'Unemployment',\n",
    "    \"Bachelor's Degree\", 'Reddit Engagement'\n",
    "], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/sample/fig_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the complete CommunityPulse pipeline:\n",
    "\n",
    "| Step | Component | Model Family |\n",
    "|---|---|---|\n",
    "| 1 | `CommunityPulseDataLoader` | — (data ingestion) |\n",
    "| 2 | `BERTEmbedder` | Week 1: Transformer NLP |\n",
    "| 3 | `TopicModeler` | Week 2: Probabilistic Topic Model |\n",
    "| 4 | `CommunityPulseAgent` | Week 3: LangChain Tool-Calling Agent |\n",
    "| 5 | `CommunityPulseAnalyser` | Qualitative Validation |\n",
    "\n",
    "**Key findings:**\n",
    "- Portland shows the most coherently negative signals across all three modalities\n",
    "- Seattle's prosperity paradox: high income but high housing anxiety on Reddit\n",
    "- Denver's challenges are less visible in news sentiment but clear in topic modeling and Census data\n",
    "\n",
    "See `blog_post.md` for the full write-up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
