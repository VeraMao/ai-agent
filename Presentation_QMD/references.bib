
@article{Casey2008,
	title = {Content-Based Music Information Retrieval: Current Directions and Future Challenges},
	volume = {96},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/4472077/},
	doi = {10.1109/JPROC.2008.916370},
	shorttitle = {Content-Based Music Information Retrieval},
	pages = {668--696},
	number = {4},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Casey, M.A. and Veltkamp, R. and Goto, M. and Leman, M. and Rhodes, C. and Slaney, M.},
	urldate = {2025-04-26},
	date = {2008-04},
}

@article{Acheampong2021,
	title = {Transformer models for text-based emotion detection: a review of {BERT}-based approaches},
	volume = {54},
	issn = {0269-2821, 1573-7462},
	url = {https://link.springer.com/10.1007/s10462-021-09958-2},
	doi = {10.1007/s10462-021-09958-2},
	shorttitle = {Transformer models for text-based emotion detection},
	pages = {5789--5829},
	number = {8},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Acheampong, Francisca Adoma and Nunoo-Mensah, Henry and Chen, Wenyu},
	urldate = {2025-04-26},
	date = {2021-12},
	langid = {english},
}

@inproceedings{Parthasarathy2017,
	location = {New Orleans, {LA}},
	title = {Ranking emotional attributes with deep neural networks},
	isbn = {978-1-5090-4117-6},
	url = {http://ieeexplore.ieee.org/document/7953107/},
	doi = {10.1109/ICASSP.2017.7953107},
	abstract = {Studies have shown that ranking emotional attributes through preference learning methods has signiﬁcant advantages over conventional emotional classiﬁcation/regression frameworks. Preference learning is particularly appealing for retrieval tasks, where the goal is to identify speech conveying target emotional behaviors (e.g., positive samples with low arousal). With recent advances in deep neural networks ({DNNs}), this study explores whether a preference learning framework relying on deep learning can outperform conventional ranking algorithms. We use a deep learning ranker implemented with the {RankNet} algorithm to evaluate preference between emotional sentences in terms of dimensional attributes (arousal, valence and dominance). The results show improved performance over ranking algorithms trained with support vector machine ({SVM}) (i.e., {RankSVM}). The results are signiﬁcantly better than performance reported in previous work, demonstrating the potential of {RankNet} to retrieve speech with target emotional behaviors.},
	eventtitle = {2017 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {4995--4999},
	booktitle = {2017 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Parthasarathy, Srinivas and Lotfian, Reza and Busso, Carlos},
	urldate = {2025-04-26},
	date = {2017-03},
	langid = {english},
}

@incollection{Longo2024,
	location = {Cham},
	title = {A Comparative Analysis of {SHAP}, {LIME}, {ANCHORS}, and {DICE} for Interpreting a Dense Neural Network in Credit Card Fraud Detection},
	volume = {2156},
	isbn = {978-3-031-63802-2 978-3-031-63803-9},
	url = {https://link.springer.com/10.1007/978-3-031-63803-9_20},
	pages = {365--383},
	booktitle = {Explainable Artificial Intelligence},
	publisher = {Springer Nature Switzerland},
	author = {Raufi, Bujar and Finnegan, Ciaran and Longo, Luca},
	editor = {Longo, Luca and Lapuschkin, Sebastian and Seifert, Christin},
	urldate = {2025-04-26},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-3-031-63803-9_20},
	note = {Series Title: Communications in Computer and Information Science},
}

@article{Russell1982,
	title = {The structure in persons' implicit taxonomy of emotions},
	volume = {16},
	issn = {0092-6566},
	url = {https://www.sciencedirect.com/science/article/pii/0092656682900058},
	doi = {https://doi.org/10.1016/0092-6566(82)90005-8},
	abstract = {Persons' classifications of emotions are viewed as based on an implicit taxonomy of emotions. This implicit taxonomy has sometimes been thought of as a bipolar dimensional system within which are located all the various emotion-descriptive categories (fear, anger, happiness, etc.). A dimensional taxonomy of this kind assumes that all emotion categories are interrelated in a systematic way. More often, however, the implicit taxonomy has been thought of as a list of separate emotions. A taxonomy in the form of a list typically presupposes that emotion categories are either synonymous, independent, or mutually exclusive. {McNair}, Lorr, and Droppleman's Profile of Mood States (San Diego: Educational and Industrial Testing Service, 1971) is one such list of emotion categories and was therefore examined in two studies for the actual interrelationships among its categories. To examine intraindividual relationships, 45 subjects rated the emotional state posed in each of 32 videotape segments. Even at the level of the individual subject, results showed that emotion categories are systematically interrelated and can be accounted for reasonably well by a system of three bipolar dimensions: pleasure-displeasure, arousal-sleepiness, and dominance-submissiveness. Evidence for the same bipolar system was also obtained in a second study, which examined interindividual differences in the self-reported emotional states of 343 subjects.},
	pages = {447--469},
	number = {4},
	journaltitle = {Journal of Research in Personality},
	author = {Russell, James A and Steiger, James H},
	date = {1982},
}

@inproceedings{Olha2023,
	title = {Method for Sentiment Analysis of Ukrainian-Language Reviews in E-Commerce Using {RoBERTa} Neural Network},
	url = {https://api.semanticscholar.org/CorpusID:258688336},
	booktitle = {International Conference on Computational Linguistics and Intelligent Systems},
	author = {{Olha Zalutska} and {Maryna Molchanova} and {Olena Sobko} and {Olexander Mazurets} and {Oleksandr Pasichnyk} and {Olexander V. Barmak} and {Iurii Krak}},
	date = {2023},
}

@inproceedings{Cano2017,
	title = {Music Mood Dataset Creation Based on Last {FM} Tags},
	isbn = {978-1-921987-66-3},
	url = {http://airccj.org/CSCP/vol7/csit76803.pdf},
	doi = {10.5121/csit.2017.70603},
	eventtitle = {Seventh International Conference on Computer Science, Engineering and Information Technology},
	pages = {15--26},
	booktitle = {Computer Science \& Information Technology ({CS} \& {IT})},
	publisher = {Academy \& Industry Research Collaboration Center ({AIRCC})},
	author = {Cano, Erion and Morisio, Maurizio},
	urldate = {2025-04-26},
	date = {2017-05-27},
}

@article{Kamenetsky1997,
	title = {Effect of Tempo and Dynamics on the Perception of Emotion in Music},
	volume = {25},
	rights = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0305-7356, 1741-3087},
	url = {https://journals.sagepub.com/doi/10.1177/0305735697252005},
	doi = {10.1177/0305735697252005},
	abstract = {Adults (N = 96) with little or no training in music heard one of four possible {MIDI} versions of each of four musical excerpts. The four versions of each excerpt included one with unvarying tempo and dynamics, one with variations in tempo only, one with variations in dynamics only, and one with variations in tempo and dynamics. Participants rated each excerpt on a 7-point scale for likeability and emotional expressiveness. Variations in dynamics resulted in higher ratings on both measures but variations in tempo had no such effect. In general, women rated the musical excerpts as more emotionally expressive and more likeable than did men. Finally, musical preferences were highly correlated with ratings of emotional expressiveness.},
	pages = {149--160},
	number = {2},
	journaltitle = {Psychology of Music},
	shortjournal = {Psychology of Music},
	author = {Kamenetsky, Stuart B. and Hill, David S. and Trehub, Sandra E.},
	urldate = {2025-04-26},
	date = {1997-10},
	langid = {english},
}

@misc{Artemova2025,
	title = {Hands-On Tutorial: Labeling with {LLM} and Human-in-the-Loop},
	url = {http://arxiv.org/abs/2411.04637},
	doi = {10.48550/arXiv.2411.04637},
	shorttitle = {Hands-On Tutorial},
	abstract = {Training and deploying machine learning models relies on a large amount of human-annotated data. As human labeling becomes increasingly expensive and time-consuming, recent research has developed multiple strategies to speed up annotation and reduce costs and human workload: generating synthetic training data, active learning, and hybrid labeling. This tutorial is oriented toward practical applications: we will present the basics of each strategy, highlight their benefits and limitations, and discuss in detail real-life case studies. Additionally, we will walk through best practices for managing human annotators and controlling the quality of the final dataset. The tutorial includes a hands-on workshop, where attendees will be guided in implementing a hybrid annotation setup. This tutorial is designed for {NLP} practitioners from both research and industry backgrounds who are involved in or interested in optimizing data labeling projects.},
	number = {{arXiv}:2411.04637},
	publisher = {{arXiv}},
	author = {Artemova, Ekaterina and Tsvigun, Akim and Schlechtweg, Dominik and Fedorova, Natalia and Chernyshev, Konstantin and Tilga, Sergei and Obmoroshev, Boris},
	urldate = {2025-04-21},
	date = {2025-01-27},
	eprinttype = {arxiv},
	eprint = {2411.04637 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{Kim2024,
	title = {{MEGAnno}+: A Human-{LLM} Collaborative Annotation System},
	url = {http://arxiv.org/abs/2402.18050},
	doi = {10.48550/arXiv.2402.18050},
	shorttitle = {{MEGAnno}+},
	abstract = {Large language models ({LLMs}) can label data faster and cheaper than humans for various {NLP} tasks. Despite their prowess, {LLMs} may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and {LLMs} work together to produce reliable and high-quality labels. We present {MEGAnno}+, a human-{LLM} collaborative annotation system that offers effective {LLM} agent and annotation management, convenient and robust {LLM} annotation, and exploratory verification of {LLM} labels by humans.},
	number = {{arXiv}:2402.18050},
	publisher = {{arXiv}},
	author = {Kim, Hannah and Mitra, Kushan and Chen, Rafael Li and Rahman, Sajjadur and Zhang, Dan},
	urldate = {2025-04-21},
	date = {2024-02-28},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@software{SpotifyAPI,
	title = {Spotify Web {API}},
	url = {https://developer.spotify.com/documentation/web-api},
}

@misc{GrosseMalte2022,
	title = {8+ M. Spotify Tracks, Genre, Audio Features},
	url = {https://www.kaggle.com/datasets/maltegrosse/8-m-spotify-tracks-genre-audio-features/data},
	author = {Grosse, Malte},
	date = {2022},
}

@inproceedings{MillionSongDataset,
	title = {The Million Song Dataset},
	url = {https://labrosa.ee.columbia.edu/millionsong},
	eventtitle = {12th International Society for Music Information Retrieval Conference},
	booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference ({ISMIR} 2011)},
	author = {Thierry, Bertin-Mahieux and Daniel, P.W. Ellis and Brian, Whitman and Paul, Lamere},
	date = {2011},
}

@software{Hartman2022,
	title = {Emotion English {DistilRoBERTa}-base},
	url = {https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/},
	author = {Hartmann, Jochen},
	date = {2022},
}

@misc{Mikolov2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	number = {{arXiv}:1301.3781},
	publisher = {{arXiv}},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2025-04-11},
	date = {2013-09-07},
	keywords = {Computer Science - Computation and Language},
}

@article{Bandhakavi2017,
	title = {Lexicon Generation for Emotion Detection from Text},
	volume = {32},
	issn = {1941-1294},
	url = {https://ieeexplore.ieee.org/abstract/document/7851145?casa_token=BP2dXjPKQEMAAAAA:WQKU9husdL6_1NViXxmikxT2WNKZjLOUuIwpCERQyoEBOYrjinE9JdqY_uMh3vNO7WNOHdsIVf0},
	doi = {10.1109/MIS.2017.22},
	abstract = {General-purpose emotion lexicons ({GPELs}) that associate words with emotion categories remain a valuable resource for emotion detection. However, the static and formal nature of their vocabularies make them an inadequate resource for detecting emotions in domains that are inherently dynamic in nature. This calls for lexicons that are not only adaptive to the lexical variations in a domain but which also provide finer-grained quantitative estimates to accurately capture word-emotion associations. In this article, the authors demonstrate how to harness labeled emotion text (such as blogs and news headlines) and weakly labeled emotion text (such as tweets) to learn a word-emotion association lexicon by jointly modeling emotionality and neutrality of words using a generative unigram mixture model ({UMM}). Empirical evaluation confirms that {UMM} generated emotion language models (topics) have significantly lower perplexity compared to those from state-of-the-art generative models like supervised Latent Dirichlet Allocation ({sLDA}). Further emotion detection tasks involving word-emotion classification and document-emotion ranking confirm that the {UMM} lexicon significantly out performs {GPELs} and also state-of-the-art domain specific lexicons.},
	pages = {102--108},
	number = {1},
	journaltitle = {{IEEE} Intelligent Systems},
	author = {Bandhakavi, Anil and Wiratunga, Nirmalie and Massie, Stewart and Padmanabhan, Deepak},
	urldate = {2025-04-06},
	date = {2017-01},
	keywords = {Emotion recognition, Intelligent systems, Mathematical model, Mixture models, Sentiment analysis, Social network services, Vocabulary, domain-specific lexicon, emotion detection, emotion ranking, intelligent systems, mixture model, word classification},
}

@inproceedings{Chauhan2024,
	location = {Pimari Chinchwad, India},
	title = {Real Time Mood Detection on Music Streaming Platforms: A Deep learning perspective},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5421-8},
	url = {https://ieeexplore.ieee.org/document/10837722/},
	doi = {10.1109/ASIANCON62057.2024.10837722},
	shorttitle = {Real Time Mood Detection on Music Streaming Platforms},
	eventtitle = {2024 4th Asian Conference on Innovation in Technology ({ASIANCON})},
	pages = {1--6},
	booktitle = {2024 4th Asian Conference on Innovation in Technology ({ASIANCON})},
	publisher = {{IEEE}},
	author = {Chauhan, Rahul and Vashisht, Anshul and Negi, Abhishek and Devliyal, Swati},
	urldate = {2025-03-30},
	date = {2024-08-23},
}

@article{Garg2022,
	title = {Machine learning model for mapping of music mood and human emotion based on physiological signals},
	volume = {81},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-021-11650-0},
	doi = {10.1007/s11042-021-11650-0},
	pages = {5137--5177},
	number = {4},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Garg, Anupam and Chaturvedi, Vybhav and Kaur, Arman Beer and Varshney, Vedansh and Parashar, Anshu},
	urldate = {2025-03-30},
	date = {2022-02},
	langid = {english},
}

@article{Rosner2018,
	title = {Automatic music genre classification based on musical instrument track separation},
	volume = {50},
	issn = {0925-9902, 1573-7675},
	url = {http://link.springer.com/10.1007/s10844-017-0464-5},
	doi = {10.1007/s10844-017-0464-5},
	pages = {363--384},
	number = {2},
	journaltitle = {Journal of Intelligent Information Systems},
	shortjournal = {J Intell Inf Syst},
	author = {Rosner, Aldona and Kostek, Bozena},
	urldate = {2025-03-30},
	date = {2018-04},
	langid = {english},
}

@article{Perlovsky2010,
	title = {Musical emotions: Functions, origins, evolution},
	volume = {7},
	issn = {1571-0645},
	url = {https://www.sciencedirect.com/science/article/pii/S1571064509000438},
	doi = {https://doi.org/10.1016/j.plrev.2009.11.001},
	pages = {2--27},
	number = {1},
	journaltitle = {Physics of Life Reviews},
	author = {Perlovsky, Leonid},
	date = {2010},
	keywords = {Cognitive dissonance, Culture, Emotions, Evolution, Knowledge instinct, Language, Mathematical models, Mind, Music, Neural mechanisms},
}

@inproceedings{Joseph2019,
	title = {Machine learning approaches for emotion classification of music: a systematic literature review},
	doi = {10.1109/ICAC49085.2019.9103378},
	pages = {334--339},
	booktitle = {2019 international conference on advancements in computing ({ICAC})},
	author = {Joseph, Charles and Lekamge, Sugeeswari},
	date = {2019},
	keywords = {Bibliographies, Databases, Emotion recognition, Feature extraction, Machine learning, Music, Systematics, machine learning, music emotion, music emotion classification, systematic literature review},
}

@article{Yoo2024,
	title = {Emotion Recognition and Multi-class Classification in Music with {MFCC} and Machine Learning},
	volume = {14},
	rights = {https://creativecommons.org/licenses/by/4.0},
	issn = {2460-6952, 2088-5334},
	url = {https://ijaseit.insightsociety.org/index.php/ijaseit/article/view/18671},
	doi = {10.18517/ijaseit.14.3.18671},
	pages = {818--825},
	number = {3},
	journaltitle = {International Journal on Advanced Science, Engineering and Information Technology},
	shortjournal = {Int. J. Adv. Sci. Eng. Inf. Technol.},
	author = {Yoo, Gilsang and Hong, Sungdae and Kim, Hyeocheol},
	urldate = {2025-03-30},
	date = {2024-06-03},
}

@inproceedings{Juthi2020,
	location = {Cham},
	title = {Music emotion recognition with the extraction of audio features using machine learning approaches},
	isbn = {978-3-030-30577-2},
	pages = {318--329},
	booktitle = {Proceedings of {ICETIT} 2019},
	publisher = {Springer International Publishing},
	author = {Juthi, Jannatul Humayra and Gomes, Anthony and Bhuiyan, Touhid and Mahmud, Imran},
	editor = {Singh, Pradeep Kumar and Panigrahi, Bijaya Ketan and Suryadevara, Nagender Kumar and Sharma, Sudhir Kumar and Singh, Amit Prakash},
	date = {2020},
}

@article{Xu2011,
	title = {Using machine learning analysis to interpret the relationship between music emotion and lyric features},
	volume = {7},
	issn = {2376-5992},
	url = {https://doi.org/10.7717/peerj-cs.785},
	doi = {10.7717/peerj-cs.785},
	pages = {e785},
	journaltitle = {{PeerJ} Computer Science},
	author = {Xu, Liang and Sun, Zaoyi and Wen, Xin and Huang, Zhengxi and Chao, Chi-ju and Xu, Liuchang},
	date = {2021-11},
	keywords = {Audio signal processing, Chinese pop song, {LIWC}, Lyric feature extraction, Music emotion recognition},
}

@article{Xia2022,
	title = {Study on music emotion recognition based on the machine learning model clustering algorithm},
	volume = {2022},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/9256586},
	doi = {https://doi.org/10.1155/2022/9256586},
	pages = {9256586},
	number = {1},
	journaltitle = {Mathematical Problems in Engineering},
	author = {Xia, Yu and Xu, Fumei},
	date = {2022},
}

@article{Li2023,
	title = {Emotion recognition of music based on machine learning scenarios},
	volume = {39},
	doi = {10.54097/hset.v39i.6515},
	pages = {144--150},
	journaltitle = {Highlights in Science, Engineering and Technology},
	author = {Li, Zenan},
	date = {2023-04},
}

@article{McCraty1998,
	title = {The effects of different types of music on mood, tension, and mental clarity},
	volume = {4},
	pages = {75--84},
	journaltitle = {Alternative therapies in health and medicine},
	author = {{McCraty}, Rollin and Barrios-Choplin, B and Atkinson, M and Tomasino, Dana},
	date = {1998-02},
}

@article{Yang2024,
	title = {Comparison and analysis of prediction accuracy between traditional machine learning algorithms and {XGBoost} algorithm in music emotion classification},
	volume = {57},
	doi = {10.54254/2755-2721/57/20241316},
	pages = {98--103},
	journaltitle = {Applied and Computational Engineering},
	author = {Yang, Mengxi},
	date = {2024-04},
}

@inproceedings{Helmholz2019,
	title = {Feel the moosic: Emotion-based music selection and recommendation},
	doi = {10.18690/978-961-286-280-0.11},
	author = {Helmholz, Patrick and Meyer, Michael and Robra-Bissantz, Susanne},
	date = {2019-06},
}

@article{Huron2015,
	title = {Affect induction through musical sounds: an ethological perspective},
	volume = {370},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2014.0098},
	doi = {10.1098/rstb.2014.0098},
	pages = {20140098},
	number = {1664},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Huron, David},
	date = {2015},
}

@article{Leubner2017,
	title = {Reviewing the effectiveness of music interventions in treating depression},
	volume = {8},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.01109},
	doi = {10.3389/fpsyg.2017.01109},
	journaltitle = {Frontiers in Psychology},
	author = {Leubner, Daniel and Hinterberger, Thilo},
	date = {2017},
}
